
import torch

K = torch.tensor([[1, 2, 3], [4, 5, 6], [11, 22, 53]])
K_transposed = K.transpose(-2, -1)

# Print the original and transposed matrices
print("Original matrix K:")
print(K)
print("\nTransposed matrix K_transposed:")
print(K_transposed)

########################################################################
# Sample attention scores (assume a sequence length of 4)
attn_scores = torch.tensor([[0.1, 0.2, 0.3, 0.4],
                           [0.5, 0.6, 0.7, 0.8],
                           [0.9, 1.0, 1.1, 1.2],
                           [1.3, 1.4, 1.5, 1.6]])

# Sample mask (assume the last two tokens are padding)
mask = torch.tensor([[1, 1, 0, 0],
                     [1, 1, 0, 0],
                     [1, 1, 0, 0],
                     [1, 1, 0, 0]])

# Apply masking
attn_scores_masked = attn_scores.masked_fill(mask == 0, -1e9)

print("Original attention scores:\n", attn_scores)
print("\nMasked attention scores:\n", attn_scores_masked)

###########################################################

# Create a 2x3x2 tensor
matrix = torch.tensor([[[1, 2], [3, 4], [5, 6]], 
                       [[7, 8], [9, 10], [11, 12]]])

# Transpose the last two dimensions (dimensions 1 and 2)
matrix_transposed = matrix.transpose(1, 2)

# Print the original and transposed matrices
print("Original matrix:")
print(matrix)
print("\nTransposed matrix:")
print(matrix_transposed)

#######################################################

"""
1. pe: - The tensor holding positional encodings.
2. :: - Selects all rows of the tensor.
3. i::n -Selects columns starting from index i (the first column) and going to the 
end with a step of n. This means it takes every other column, starting with the first one.

"""

torch.arange(0, 20, 2).float()
pe = torch.rand(4, 8)
print(pe)
print(pe[:, 1::2])

############################################################



"""
src_mask = (src != 0).unsqueeze(1).unsqueeze(2):

    Creates a mask for the source sequence where True corresponds to real tokens and False to padding tokens.
    unsqueeze(1) and unsqueeze(2) add extra dimensions to match the shape expected by the attention mechanism.

tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3):

    Similarly, creates a mask for the target sequence, indicating real tokens and padding.

seq_length = tgt.size(1):

    Gets the length of the target sequence.

    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool():

    Generates a "no-peeking" mask to prevent the model from attending to future tokens in the target sequence during training.
    torch.triu creates an upper triangular matrix, which is then subtracted from 1 and converted to Boolean.

tgt_mask = tgt_mask & nopeak_mask:

    Combines the target padding mask with the no-peeking mask to create the final target mask.

return src_mask, tgt_mask:

    Returns both the source and target masks.

"""



def get_mask(src, tgt):
  src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
  tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)
  print("tgt_mask in function: ", tgt_mask)
  print("tgt_mask shape in function: ", tgt_mask.shape)
  print("src_mask shape in function: ", src_mask.shape)
  seq_length = tgt.size(1)
  print("seq_length in function: ", seq_length)
  nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()
  print("nopeak_mask in function: ", nopeak_mask)
  print("nopeak_mask shape in function: ", nopeak_mask.shape)
  tgt_mask = tgt_mask & nopeak_mask
  
  return src_mask, tgt_mask


# Sample source and target sequences
src = torch.tensor([[1, 2, 3, 0], [4, 5, 0, 0]])
tgt = torch.tensor([[6, 7, 8, 9], [10, 11, 0, 0]])

print("src: ", src)
print("tgt: ", tgt)
# Generate masks
src_mask, tgt_mask = get_mask(src, tgt)

print("Source Mask:\n", src_mask)
print("\nTarget Mask:\n", tgt_mask)


Print(" upper triangular ", (1 - torch.triu(torch.ones(1, 5, 5 ), diagonal =1)))

#####################################################
