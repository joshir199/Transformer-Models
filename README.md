# Transformer-Models
Understanding and Creating various transformer models for NLP task


Transformer model was introduced in the paper titled "Attention is all you need" by Vaswani et al. The core idea behind it is the attention mechanism, specifically the concept of 'self-attention,' which allows the model to weigh and prioritize different parts of the input data and focus on significant part while producing outputs.

Full code implementation with proper explanation is attached in this repo, check it ![here](https://github.com/joshir199/Transformer-Models/tree/main/transformer_from_scratch)

*************************************
# Transformer Architecture


![](https://github.com/joshir199/Transformer-Models/blob/main/transformer_from_scratch/images/The-Transformer-model-architecture.png)


Original paper: [Attention Is All You Need]: https://arxiv.org/abs/1706.03762
********************************

# Further reading for detailed understanding:
1. https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html
2. https://jalammar.github.io/illustrated-transformer/
3. https://nlp.seas.harvard.edu/2018/04/03/attention.html
4. https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/


